{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include dependencies in PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Include dependencies in PATH\n",
    "import sys\n",
    "sys.path.append('/home/stud22015337/Desktop/Ivan/lib/python3.8/site-packages')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import csv\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(file):\n",
    "    row_numbers = []\n",
    "    split = []\n",
    "\n",
    "    for idx, value in enumerate(file.iloc[:,0]):\n",
    "        if pd.notnull(value):\n",
    "            row_numbers.append(idx)\n",
    "\n",
    "    row_numbers.append(\"END\")\n",
    "\n",
    "    for index, item in enumerate(row_numbers):\n",
    "        if row_numbers[index+1] == \"END\":\n",
    "            no_of_rows = 0\n",
    "            slice_df = file.iloc[row_numbers[index]:row_numbers[index]+1,:]\n",
    "\n",
    "            # Check if the slice is empty\n",
    "            if not slice_df.empty:\n",
    "                while len(slice_df) > 0:\n",
    "                    slice_df = file.iloc[row_numbers[index]+no_of_rows:row_numbers[index]+1+no_of_rows,:]\n",
    "                    no_of_rows += 1\n",
    "                no_of_rows -= 1\n",
    "                split.append(file.iloc[row_numbers[index]:row_numbers[index]+no_of_rows,:])\n",
    "            break\n",
    "        else:\n",
    "            new_x = row_numbers[index+1]\n",
    "            slice_df = file.iloc[item:new_x,:]\n",
    "\n",
    "            # Check if the slice is empty\n",
    "            if not slice_df.empty:\n",
    "                split.append(slice_df)\n",
    "\n",
    "    return split\n",
    "\n",
    "def findbydate(data, month):\n",
    "    date_1 = dt.datetime.now()\n",
    "    print(\"[+] Current Date: \", date_1)\n",
    "    arr = []\n",
    "    \n",
    "    for index, i in enumerate(data):\n",
    "        # Assuming `data` is a list of pandas DataFrames or similar structures\n",
    "        event_date = i.iloc[0]['date']\n",
    "#         print(event_date,index)\n",
    "        if pd.isna(event_date):  # Check if the date is NaN\n",
    "#             print(f\"[+] Event date is non-existent in dataset, please check id\", i.iloc[0,0])\n",
    "            test=\"\"\n",
    "        else:\n",
    "            event_date_str = str(event_date)\n",
    "#             print(event_date_str, index)\n",
    "\n",
    "            try:\n",
    "                # Try parsing date in %d/%m/%Y format\n",
    "                date_2 = dt.datetime.strptime(event_date_str, '%d/%m/%Y')\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    # If parsing fails, try parsing date in %Y-%m-%d format\n",
    "                    date_2 = dt.datetime.strptime(event_date_str, '%Y-%m-%d')\n",
    "                except ValueError as e:\n",
    "                    print(f\"[+] Error parsing date at index {index}: {e}\")\n",
    "                    continue\n",
    "            difference = relativedelta(date_1, date_2)\n",
    "            calculate_difference = difference.years * 12 + difference.months\n",
    "            if calculate_difference < month:\n",
    "                arr.append(i)\n",
    "    \n",
    "    print(\"[+] Successfully filtered by date\")\n",
    "    return arr\n",
    "\n",
    "def ioccounter(dataset, column_name):\n",
    "\n",
    "    total_counts = {}\n",
    "    \n",
    "    for data in dataset:\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise ValueError(\"Each element in dataset should be a DataFrame\")\n",
    "        # Check if 'Attribute_type' column exists in the DataFrame\n",
    "        if 'attribute_type' not in data.columns:\n",
    "            raise ValueError(\"DataFrame does not have 'Attribute_type' column\")\n",
    "        \n",
    "        # Filter the DataFrame by column_name\n",
    "        column_df = data[data['attribute_type'] == column_name]\n",
    "        \n",
    "        # Check if 'Attribute_value' column exists in the filtered DataFrame\n",
    "        if 'attribute_value' not in column_df.columns:\n",
    "            raise ValueError(f\"DataFrame does not have 'Attribute_value' column after filtering by {column_name}\")\n",
    "        \n",
    "        # Count occurrences of each value in 'Attribute_value'\n",
    "        column_counts = column_df['attribute_value'].value_counts().to_dict()\n",
    "        \n",
    "        # Update total_counts with counts from current DataFrame\n",
    "        for value, count in column_counts.items():\n",
    "            if value in total_counts:\n",
    "                total_counts[value] += count\n",
    "            else:\n",
    "                total_counts[value] = count\n",
    "        sorted_total_counts = dict(sorted(total_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "    return sorted_total_counts\n",
    "\n",
    "def combine_csv(directory,filename):\n",
    "    arr = pd.DataFrame()\n",
    "    files = os.listdir(directory)\n",
    "    for i in range(1,len(files),1):\n",
    "        filenamefinal = directory+\"/\"+filename+str(i)+\".csv\"\n",
    "        df = pd.read_csv(filenamefinal,low_memory=False)\n",
    "        arr = pd.concat([arr,df])\n",
    "        print(\"[+] Successfully combined filename\",filenamefinal)\n",
    "    return arr\n",
    "\n",
    "def baseline1(row):\n",
    "    return {\n",
    "        'attribute_id': row['attribute_id'], \n",
    "        'attribute_type': row['attribute_type'], \n",
    "        'attribute_category': row['attribute_category'], \n",
    "        'attribute_value': row['attribute_value'],\n",
    "        'country_name': row['Country Name'],\n",
    "        'latitude': row['Latitude'],\n",
    "        'longitude': row['Longitude']  \n",
    "    }\n",
    "def cisareader(arr):\n",
    "    for i in arr:\n",
    "        test = i.iloc[:,27:42]\n",
    "        print(type(test))\n",
    "        break\n",
    "    \n",
    "        \n",
    "def createcombinedcsv(arr,filename):\n",
    "    # check for existing file with same name\n",
    "    os.listdir()\n",
    "    # if yes, delete file\n",
    "    if os.path.isfile(filename):\n",
    "    # If the file exists, delete it\n",
    "        os.remove(filename)\n",
    "        print(f\"{filename} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"{filename} does not exist.\")\n",
    "    \n",
    "    \n",
    "    # For charlene grafana\n",
    "    for index,data in enumerate(arr):\n",
    "        if index==0:\n",
    "            data.to_csv(filename,index=False,mode='a')\n",
    "        else:\n",
    "            data.to_csv(filename,index=False,mode='a',header=False)\n",
    "    print(\"[+] New dataset combined done\")\n",
    "\n",
    "# Used to extract solely the CISA table, currently not in use \n",
    "    \n",
    "# def cisareader(arr):\n",
    "#     df = pd.DataFrame(arr)\n",
    "#     columns_to_extract = ['icsad_ID','icsad_ID',\n",
    "#        'Original_Release_Date', 'Last_Updated', 'Year', 'ICS-CERT_Number',\n",
    "#        'ICS-CERT_Advisory_Title', 'Vendor', 'Product', 'Products_Affected',\n",
    "#        'CVE_Number', 'Cumulative_CVSS', 'CVSS_Severity', 'CWE_Number',\n",
    "#        'Critical_Infrastructure_Sector', 'Product_Distribution',\n",
    "#        'Company_Headquarters','EPSS Scores','Percentiles','Dates']\n",
    "#     result = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         row_data = [row[col] for col in columns_to_extract]\n",
    "#         result.append(row_data)\n",
    "#     return result\n",
    "\n",
    "def custom_date_parser(date_string):\n",
    "    # Use a custom date parser to handle specific formats\n",
    "    return pd.to_datetime(date_string, format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main (MISP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Successfully combined filename official/official_part_1.csv\n",
      "[+] Successfully combined filename official/official_part_2.csv\n",
      "[+] Successfully combined filename official/official_part_3.csv\n",
      "[+] Successfully combined filename official/official_part_4.csv\n",
      "[+] Successfully combined filename official/official_part_5.csv\n",
      "[+] Successfully combined filename official/official_part_6.csv\n",
      "[+] Successfully combined filename official/official_part_7.csv\n",
      "[+] Successfully combined filename official/official_part_8.csv\n",
      "[+] Successfully combined filename official/official_part_9.csv\n",
      "[+] Successfully combined filename official/official_part_10.csv\n",
      "[+] Successfully combined filename official/official_part_11.csv\n",
      "[+] Current Date:  2024-07-25 23:15:45.030072\n",
      "[+] Successfully filtered by date\n",
      "combined_final.csv has been deleted.\n",
      "[+] New dataset combined done\n"
     ]
    }
   ],
   "source": [
    "arr = combine_csv(\"official\",\"official_part_\")\n",
    "df = readfile(arr)\n",
    "datefilter = findbydate(df,6) # filter data by months \n",
    "#make cleanser function here \n",
    "\n",
    "# count = ioccounter(testing,\"ip-dst\")\n",
    "createcombinedcsv(df,\"combined_final.csv\") # this is for charlwene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events:  17550\n",
      "Number of events filtered by date: 4900\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of events: \",len(df))\n",
    "print(\"Number of events filtered by date:\",len(datefilter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CISA + EPSS handling (Currently not in use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cisa = cisareader(arr)\n",
    "\n",
    "# filename = 'cisa.csv'\n",
    "\n",
    "# with open(filename, 'w', newline='') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     # Write each list as a row\n",
    "#     for row in cisa:\n",
    "#         writer.writerow(row)\n",
    "\n",
    "# print(f\"Data written to {filename} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Creating a baseline (IP-src addresses baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess-ip-src.csv has been deleted.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in df:\n",
    "    eventid = i.iloc[0, 0]\n",
    "    date = i.iloc[0, 1]\n",
    "    orgid = i.iloc[0, 2]\n",
    "    orgname = i.iloc[0, 3]\n",
    "    orgcid = i.iloc[0, 4]\n",
    "    orgcname = i.iloc[0, 5]\n",
    "    eventname = i.iloc[0, 6]\n",
    "    threatlevel = i.iloc[0, 7]\n",
    "    time = i.iloc[0, 9]\n",
    "    \n",
    "    # Apply the baseline1 function\n",
    "    test = i.apply(baseline1, axis=1)\n",
    "   \n",
    "    for index, row in test.items():\n",
    "        result_row = {\n",
    "            'eventid': eventid, \n",
    "            'date': date, \n",
    "            'orgid': orgid, \n",
    "            'orgname': orgname, \n",
    "            'orgcid': orgcid, \n",
    "            'orgcname': orgcname, \n",
    "            'eventname': eventname, \n",
    "            'threatlevel': threatlevel, \n",
    "            'time': time\n",
    "        }\n",
    "        \n",
    "        # Update the result_row dictionary with values returned by baseline1\n",
    "        result_row.update(row)\n",
    "        if result_row['attribute_type'] == 'ip-src':\n",
    "            results.append(result_row)\n",
    "\n",
    "# Convert the results list to a DataFrame and save to CSV\n",
    "final_df = pd.DataFrame(results)\n",
    "if os.path.isfile(\"preprocess-ip-src.csv\"):\n",
    "# If the file exists, delete it\n",
    "    os.remove(\"preprocess-ip-src.csv\")\n",
    "    print(\"preprocess-ip-src.csv has been deleted.\")\n",
    "else:\n",
    "    print(\"preprocess-ip-src.csv does not exist.\")\n",
    "final_df.to_csv(\"preprocess-ip-src.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP-src anomalies.csv has been deleted.\n",
      "        Unnamed: 0  eventid       date  orgid                       orgname  \\\n",
      "0                0      1.0  2/10/2014    1.0                        OTISAC   \n",
      "1                1      1.0  2/10/2014    1.0                        OTISAC   \n",
      "2                2      1.0  2/10/2014    1.0                        OTISAC   \n",
      "3                3      1.0  2/10/2014    1.0                        OTISAC   \n",
      "4                4      1.0  2/10/2014    1.0                        OTISAC   \n",
      "...            ...      ...        ...    ...                           ...   \n",
      "109645      109645  19293.0  16/7/2024    1.0                        OTISAC   \n",
      "109646      109646  19293.0  16/7/2024    1.0                        OTISAC   \n",
      "109689      109689  19432.0  24/3/2023   25.0  OT-ISAC Member MISP Platform   \n",
      "109715      109715  19522.0  14/4/2023   25.0  OT-ISAC Member MISP Platform   \n",
      "109729      109729  19536.0  25/4/2023   25.0  OT-ISAC Member MISP Platform   \n",
      "\n",
      "        orgcid        orgcname  \\\n",
      "0          2.0  CthulhuSPRL.be   \n",
      "1          2.0  CthulhuSPRL.be   \n",
      "2          2.0  CthulhuSPRL.be   \n",
      "3          2.0  CthulhuSPRL.be   \n",
      "4          2.0  CthulhuSPRL.be   \n",
      "...        ...             ...   \n",
      "109645     1.0          OTISAC   \n",
      "109646     1.0          OTISAC   \n",
      "109689   242.0      SOC OneseQ   \n",
      "109715   242.0      SOC OneseQ   \n",
      "109729   242.0      SOC OneseQ   \n",
      "\n",
      "                                                eventname  threatlevel  \\\n",
      "0              OSINT ShellShock scanning IPs from OpenDNS          3.0   \n",
      "1              OSINT ShellShock scanning IPs from OpenDNS          3.0   \n",
      "2              OSINT ShellShock scanning IPs from OpenDNS          3.0   \n",
      "3              OSINT ShellShock scanning IPs from OpenDNS          3.0   \n",
      "4              OSINT ShellShock scanning IPs from OpenDNS          3.0   \n",
      "...                                                   ...          ...   \n",
      "109645  Check Point: New BUGSLEEP Backdoor Deployed in...          2.0   \n",
      "109646  Check Point: New BUGSLEEP Backdoor Deployed in...          2.0   \n",
      "109689                   Dasan.GPON.Remote.Code.Execution          2.0   \n",
      "109715                   Dasan.GPON.Remote.Code.Execution          2.0   \n",
      "109729  Multiple.Routers.GPON.formLogin.Remote.Command...          2.0   \n",
      "\n",
      "                             time  ...  orgname_encoded orgcid_encoded  \\\n",
      "0       2018-02-05T07:50:37+00:00  ...                1              1   \n",
      "1       2018-02-05T07:50:37+00:00  ...                1              1   \n",
      "2       2018-02-05T07:50:37+00:00  ...                1              1   \n",
      "3       2018-02-05T07:50:37+00:00  ...                1              1   \n",
      "4       2018-02-05T07:50:37+00:00  ...                1              1   \n",
      "...                           ...  ...              ...            ...   \n",
      "109645  2024-07-16T02:52:48+00:00  ...                1              0   \n",
      "109646  2024-07-16T02:52:48+00:00  ...                1              0   \n",
      "109689  2024-07-15T09:27:47+00:00  ...                0             78   \n",
      "109715  2024-07-17T07:49:50+00:00  ...                0             78   \n",
      "109729  2024-07-17T08:54:41+00:00  ...                0             78   \n",
      "\n",
      "       orgcname_encoded eventname_encoded threatlevel_encoded  time_encoded  \\\n",
      "0                    22               614                   2            23   \n",
      "1                    22               614                   2            23   \n",
      "2                    22               614                   2            23   \n",
      "3                    22               614                   2            23   \n",
      "4                    22               614                   2            23   \n",
      "...                 ...               ...                 ...           ...   \n",
      "109645               45               237                   1          1246   \n",
      "109646               45               237                   1          1246   \n",
      "109689               52               316                   1          1241   \n",
      "109715               52               316                   1          1250   \n",
      "109729               52               561                   1          1266   \n",
      "\n",
      "        attributeid_encoded  attributetype_encoded  attributecategory_encoded  \\\n",
      "0                         0                      0                          0   \n",
      "1                         1                      0                          0   \n",
      "2                         2                      0                          0   \n",
      "3                         3                      0                          0   \n",
      "4                         4                      0                          0   \n",
      "...                     ...                    ...                        ...   \n",
      "109645                94157                      0                          0   \n",
      "109646                94158                      0                          0   \n",
      "109689                94201                      0                          0   \n",
      "109715                94227                      0                          0   \n",
      "109729                94241                      0                          0   \n",
      "\n",
      "        countryname_encoded  \n",
      "0                        39  \n",
      "1                        91  \n",
      "2                       171  \n",
      "3                       171  \n",
      "4                        39  \n",
      "...                     ...  \n",
      "109645                  185  \n",
      "109646                  142  \n",
      "109689                  185  \n",
      "109715                  141  \n",
      "109729                   81  \n",
      "\n",
      "[10958 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv(\"preprocess-ip-src.csv\",low_memory=False)\n",
    "\n",
    "# Assuming 'value' is the column with numeric data for anomaly detection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming attribute_value is a categorical column in your DataFrame\n",
    "encoder = LabelEncoder()\n",
    "data['attribute_value_encoded'] = encoder.fit_transform(data['attribute_value'])\n",
    "data['eventid_encoded'] = encoder.fit_transform(data['eventid'])\n",
    "data['date_encoded'] = encoder.fit_transform(data['date'])\n",
    "data['orgid_encoded'] = encoder.fit_transform(data['orgid'])\n",
    "data['orgname_encoded'] = encoder.fit_transform(data['orgname'])\n",
    "data['orgcid_encoded'] = encoder.fit_transform(data['orgcid'])\n",
    "data['orgcname_encoded'] = encoder.fit_transform(data['orgcname'])\n",
    "data['eventname_encoded'] = encoder.fit_transform(data['eventname'])\n",
    "data['threatlevel_encoded'] = encoder.fit_transform(data['threatlevel'])\n",
    "data['time_encoded'] = encoder.fit_transform(data['time'])\n",
    "data['attributeid_encoded'] = encoder.fit_transform(data['attribute_id'])\n",
    "data['attributetype_encoded'] = encoder.fit_transform(data['attribute_type'])\n",
    "data['attributecategory_encoded'] = encoder.fit_transform(data['attribute_category'])\n",
    "data['countryname_encoded'] = encoder.fit_transform(data['country_name'])\n",
    "\n",
    "# Now use 'attribute_value_encoded' for anomaly detection\n",
    "X = data[['attribute_value_encoded']]\n",
    "\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "model = IsolationForest(contamination=0.1)  # Adjust contamination based on expected outlier percentage\n",
    "\n",
    "# Fit model and predict anomalies\n",
    "model.fit(X)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Anomalies are where predictions == -1\n",
    "anomalies = data[predictions == -1]\n",
    "\n",
    "# Optionally, visualize or further analyze anomalies\n",
    "if os.path.isfile(\"IP-src anomalies.csv\"):\n",
    "# If the file exists, delete it\n",
    "    os.remove(\"IP-src anomalies.csv\")\n",
    "    print(\"IP-src anomalies.csv has been deleted.\")\n",
    "else:\n",
    "    print(\"IP-src anomalies.csv does not exist.\")\n",
    "anomalies.to_csv(\"IP-src anomalies.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a baseline (IP-dst addresses baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess-ip-dst.csv has been deleted.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in df:\n",
    "    eventid = i.iloc[0, 0]\n",
    "    date = i.iloc[0, 1]\n",
    "    orgid = i.iloc[0, 2]\n",
    "    orgname = i.iloc[0, 3]\n",
    "    orgcid = i.iloc[0, 4]\n",
    "    orgcname = i.iloc[0, 5]\n",
    "    eventname = i.iloc[0, 6]\n",
    "    threatlevel = i.iloc[0, 7]\n",
    "    time = i.iloc[0, 9]\n",
    "    \n",
    "    # Apply the baseline1 function\n",
    "    test = i.apply(baseline1, axis=1)\n",
    "   \n",
    "    for index, row in test.items():\n",
    "        result_row = {\n",
    "            'eventid': eventid, \n",
    "            'date': date, \n",
    "            'orgid': orgid, \n",
    "            'orgname': orgname, \n",
    "            'orgcid': orgcid, \n",
    "            'orgcname': orgcname, \n",
    "            'eventname': eventname, \n",
    "            'threatlevel': threatlevel, \n",
    "            'time': time\n",
    "        }\n",
    "        \n",
    "        # Update the result_row dictionary with values returned by baseline1\n",
    "        result_row.update(row)\n",
    "        if result_row['attribute_type'] == 'ip-dst':\n",
    "            results.append(result_row)\n",
    "\n",
    "# Convert the results list to a DataFrame and save to CSV\n",
    "final_df = pd.DataFrame(results)\n",
    "if os.path.isfile(\"preprocess-ip-dst.csv\"):\n",
    "# If the file exists, delete it\n",
    "    os.remove(\"preprocess-ip-dst.csv\")\n",
    "    print(\"preprocess-ip-dst.csv has been deleted.\")\n",
    "else:\n",
    "    print(\"preprocess-ip-dst.csv does not exist.\")\n",
    "final_df.to_csv(\"preprocess-ip-dst.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP-dst anomalies.csv has been deleted.\n",
      "        Unnamed: 0  eventid        date  orgid                       orgname  \\\n",
      "1                1      2.0   3/10/2014    1.0                        OTISAC   \n",
      "6                6      4.0   9/10/2014    1.0                        OTISAC   \n",
      "11              11      4.0   9/10/2014    1.0                        OTISAC   \n",
      "21              21      6.0  11/10/2014    1.0                        OTISAC   \n",
      "37              37      6.0  11/10/2014    1.0                        OTISAC   \n",
      "...            ...      ...         ...    ...                           ...   \n",
      "156642      156642  19573.0   5/11/2021   25.0  OT-ISAC Member MISP Platform   \n",
      "156643      156643  19573.0   5/11/2021   25.0  OT-ISAC Member MISP Platform   \n",
      "156644      156644  19574.0   5/11/2021   25.0  OT-ISAC Member MISP Platform   \n",
      "156645      156645  19575.0   26/5/2021   25.0  OT-ISAC Member MISP Platform   \n",
      "156646      156646  19575.0   26/5/2021   25.0  OT-ISAC Member MISP Platform   \n",
      "\n",
      "        orgcid        orgcname  \\\n",
      "1          2.0  CthulhuSPRL.be   \n",
      "6          2.0  CthulhuSPRL.be   \n",
      "11         2.0  CthulhuSPRL.be   \n",
      "21         2.0  CthulhuSPRL.be   \n",
      "37         2.0  CthulhuSPRL.be   \n",
      "...        ...             ...   \n",
      "156642    80.0      OpenCTI.BR   \n",
      "156643    80.0      OpenCTI.BR   \n",
      "156644    80.0      OpenCTI.BR   \n",
      "156645    80.0      OpenCTI.BR   \n",
      "156646    80.0      OpenCTI.BR   \n",
      "\n",
      "                                                eventname  threatlevel  \\\n",
      "1       OSINT New Indicators of Compromise for APT Gro...          2.0   \n",
      "6       OSINT Democracy in Hong Kong Under Attack blog...          2.0   \n",
      "11      OSINT Democracy in Hong Kong Under Attack blog...          2.0   \n",
      "21      OSINT Shellshock exploitation from Red Sky Wee...          3.0   \n",
      "37      OSINT Shellshock exploitation from Red Sky Wee...          3.0   \n",
      "...                                                   ...          ...   \n",
      "156642                     Cuckoo Sandbox analysis #83509          4.0   \n",
      "156643                     Cuckoo Sandbox analysis #83509          4.0   \n",
      "156644                     Cuckoo Sandbox analysis #83490          4.0   \n",
      "156645                     Cuckoo Sandbox analysis #33317          4.0   \n",
      "156646                     Cuckoo Sandbox analysis #33317          4.0   \n",
      "\n",
      "                             time  ...  orgname_encoded orgcid_encoded  \\\n",
      "1       2014-10-06T07:12:57+00:00  ...                1              1   \n",
      "6       2014-10-13T08:17:38+00:00  ...                1              1   \n",
      "11      2014-10-13T08:17:38+00:00  ...                1              1   \n",
      "21      2014-10-14T09:53:20+00:00  ...                1              1   \n",
      "37      2014-10-14T09:53:20+00:00  ...                1              1   \n",
      "...                           ...  ...              ...            ...   \n",
      "156642  2023-06-28T18:48:10+00:00  ...                0             60   \n",
      "156643  2023-06-28T18:48:10+00:00  ...                0             60   \n",
      "156644  2023-06-28T18:49:14+00:00  ...                0             60   \n",
      "156645  2023-06-28T18:36:13+00:00  ...                0             60   \n",
      "156646  2023-06-28T18:36:13+00:00  ...                0             60   \n",
      "\n",
      "       orgcname_encoded eventname_encoded threatlevel_encoded  time_encoded  \\\n",
      "1                    39              1711                   1             0   \n",
      "6                    39              1674                   1             1   \n",
      "11                   39              1674                   1             1   \n",
      "21                   39              1731                   2             2   \n",
      "37                   39              1731                   2             2   \n",
      "...                 ...               ...                 ...           ...   \n",
      "156642               91               601                   3          2141   \n",
      "156643               91               601                   3          2141   \n",
      "156644               91               600                   3          2142   \n",
      "156645               91               332                   3          2133   \n",
      "156646               91               332                   3          2133   \n",
      "\n",
      "        attributeid_encoded  attributetype_encoded  attributecategory_encoded  \\\n",
      "1                         1                      0                          1   \n",
      "6                         6                      0                          1   \n",
      "11                       11                      0                          0   \n",
      "21                       21                      0                          1   \n",
      "37                       37                      0                          1   \n",
      "...                     ...                    ...                        ...   \n",
      "156642               140018                      0                          1   \n",
      "156643               140019                      0                          1   \n",
      "156644               140020                      0                          1   \n",
      "156645               140021                      0                          1   \n",
      "156646               140022                      0                          1   \n",
      "\n",
      "        countryname_encoded  \n",
      "1                       166  \n",
      "6                       178  \n",
      "11                      178  \n",
      "21                      182  \n",
      "37                        7  \n",
      "...                     ...  \n",
      "156642                   62  \n",
      "156643                   62  \n",
      "156644                   62  \n",
      "156645                   62  \n",
      "156646                   32  \n",
      "\n",
      "[78325 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv(\"preprocess-ip-dst.csv\",low_memory=False)\n",
    "\n",
    "# Assuming 'value' is the column with numeric data for anomaly detection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming attribute_value is a categorical column in your DataFrame\n",
    "encoder = LabelEncoder()\n",
    "data['attribute_value_encoded'] = encoder.fit_transform(data['attribute_value'])\n",
    "data['eventid_encoded'] = encoder.fit_transform(data['eventid'])\n",
    "data['date_encoded'] = encoder.fit_transform(data['date'])\n",
    "data['orgid_encoded'] = encoder.fit_transform(data['orgid'])\n",
    "data['orgname_encoded'] = encoder.fit_transform(data['orgname'])\n",
    "data['orgcid_encoded'] = encoder.fit_transform(data['orgcid'])\n",
    "data['orgcname_encoded'] = encoder.fit_transform(data['orgcname'])\n",
    "data['eventname_encoded'] = encoder.fit_transform(data['eventname'])\n",
    "data['threatlevel_encoded'] = encoder.fit_transform(data['threatlevel'])\n",
    "data['time_encoded'] = encoder.fit_transform(data['time'])\n",
    "data['attributeid_encoded'] = encoder.fit_transform(data['attribute_id'])\n",
    "data['attributetype_encoded'] = encoder.fit_transform(data['attribute_type'])\n",
    "data['attributecategory_encoded'] = encoder.fit_transform(data['attribute_category'])\n",
    "data['countryname_encoded'] = encoder.fit_transform(data['country_name'])\n",
    "\n",
    "# Now use 'attribute_value_encoded' for anomaly detection\n",
    "X = data[['attribute_value_encoded']]\n",
    "\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "model = IsolationForest(contamination=0.5)  # Adjust contamination based on expected outlier percentage\n",
    "\n",
    "# Fit model and predict anomalies\n",
    "model.fit(X)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Anomalies are where predictions == 1, to find ip addresses that keep getting attacked\n",
    "anomalies = data[predictions == 1]\n",
    "\n",
    "# Optionally, visualize or further analyze anomalies\n",
    "if os.path.isfile(\"IP-dst anomalies.csv\"):\n",
    "# If the file exists, delete it\n",
    "    os.remove(\"IP-dst anomalies.csv\")\n",
    "    print(\"IP-dst anomalies.csv has been deleted.\")\n",
    "else:\n",
    "    print(\"IP-dst anomalies.csv does not exist.\")\n",
    "anomalies.to_csv(\"IP-dst anomalies.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
